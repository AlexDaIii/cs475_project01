There are no unlabled data so we don't have to pre-process that type of data

bio.train - 284 features, 284 useful, 2000 training examples, NO normalization bc higher accuracy
    (standardize) max: 44.71017781221589, min: -1.0191839793138975
    WITHOUT Standardization
    Accuracy Train: 0.991000 (1982/2000)
    Accuracy Dev: 0.965000 (193/200)
    Accuracy Test: 0.000000 (0/222)

    WITH Standardization
    Accuracy Train: 0.920000 (1840/2000)
    Accuracy Dev: 0.755000 (151/200)
    Accuracy Test: 0.000000 (0/222)  - the ys are all -1

easy.train - 10 features, 10 useful, 900 examples, standardization makes no difference
    (standardize) max: 3.771062324022551, min: -3.629691951551705
    WITHOUT Standardization
    Accuracy Train: 1.000000 (900/900)
    Accuracy Dev: 1.000000 (100/100)

    WITH Standardization
    Accuracy: 1.000000 (900/900)
    Accuracy: 1.000000 (100/100)

finance.train - 46 features, 46 useful, 550 training examples,
    (standardize) max: 17.95894130677154, min: -3.349958540373614

    WITHOUT Standardization
    Accuracy: 0.643636 (354/550)
    Accuracy: 0.866667 (26/30)
    Accuracy: 0.000000 (0/73)

    WITH Standardization
    Accuracy: 0.843636 (464/550)
    Accuracy: 0.700000 (21/30)
    Accuracy: 0.000000 (0/73)

hard.train - 94 features, 94 useful, 900 examples, standardize data
    (standardize) max: 4.38436264109483, min: -4.726133990766483

    WITHOUT Standardization
    Accuracy: 0.534444 (481/900)
    Accuracy: 0.540000 (54/100)

    WITH Standardization
    Accuracy: 0.548889 (494/900)
    Accuracy: 0.450000 (45/100)

nlp.train - 54471 features, 1000 examples, normalize (max X is 31)
    (standardize) max: 31.606961258558965, min: -0.92040958659559
    very slow if we have regularization - probably because we initialize an identity matrix that is like 54000 big

    WITHOUT Standardization
    Accuracy: 0.998000 (998/1000)
    Accuracy: 0.786667 (236/300)
    Accuracy: 0.000000 (0/300)

    WITH Standardization
    NA - too slow

speech.train - 617 features, 617 useful, 400 training examples, standardize?
    (standardize) max: 17.972433664476377, min: -6.983893867834724
    WITHOUT Standardization
    Accuracy: 0.925000 (370/400)
    Accuracy: 0.850000 (85/100)
    Accuracy: 0.000000 (0/99)

    WITH Standardization
    Accuracy: 0.992500 (397/400)
    Accuracy: 0.900000 (90/100)
    Accuracy: 0.000000 (0/99)

vision.train - 19 features, 18 useful, 500 training examples
    (standardize) max: 11.668342100980727, min: -5.723733151147001
    do we remove column? - it messes with normalization
    WITHOUT Standardization
    Accuracy: 0.994000 (497/500)
    Accuracy: 0.987500 (79/80)
    Accuracy: 0.000000 (0/80)

    WITH Standardization
    Accuracy: 0.996000 (498/500)
    Accuracy: 0.987500 (79/80)
    Accuracy: 0.000000 (0/80)

